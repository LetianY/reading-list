# Reading List
This reading list is maintained for personal record.

### 2022 July
1. [Modeling Non-linear Least Squares](http://ceres-solver.org/nnls_modeling.html). This one is about Ceres solver.
2. [A General and Adaptive Robust Loss Function](https://openaccess.thecvf.com/content_CVPR_2019/papers/Barron_A_General_and_Adaptive_Robust_Loss_Function_CVPR_2019_paper.pdf). This one gives a general form of all sorts of robust loss functions. The github repo can be found [here](https://github.com/google-research/google-research/tree/master/robust_loss).
3. [Conditional Number](https://en.wikipedia.org/wiki/Condition_number). This wiki discusses the **well-conditioned** and **ill-conditioned** case.
4. [A robust hybrid of lasso and ridge regression](https://artowen.su.domains/reports/hhu.pdf). This is a paper about huber regression. The sklearn repo can be found [here](https://github.com/scikit-learn/scikit-learn/blob/baf0ea25d/sklearn/linear_model/_huber.py#L126).
5. [Exploring Bayesian Optimization](https://distill.pub/2020/bayesian-optimization/). This is a good tutorial for bayesian optimization. But I haven't got time to read another [paper](https://arxiv.org/pdf/1807.02811.pdf) about this.
6. [GRACGE: Graph Signal Clustering and Multiple Graph Estimation](https://ieeexplore.ieee.org/document/9756907). I encountered this paper in the group meeting this month. The proof utilized many techniques I learnt in SEEM5380 (optimization methods for high-dimensional statistics).
7. [Pari-Mutuel Markets: Mechanisms and Performance](https://web.stanford.edu/~yyye/scpmfinal.pdf). This work talks about different mechanisms in the pari-mutuel market. It gives a good explanation for readers to understand how the optimization problem is formulated.
8. Regular expression: I started to use regular expression systematically this month. Here are some useful tutorial webpages for reference. 
  - [Chinese Version Introduction](https://blog.techbridge.cc/2020/05/14/introduction-to-regular-expression/)
  - [English Tutorial](https://refrf.dev/)
  - [Python Library](https://docs.python.org/3/library/re.html)
  - [Kaggle Notebook for python re](https://www.kaggle.com/code/gauravduttakiit/regular-expressions/notebook)
9. [NGBoost](https://stanfordmlgroup.github.io/projects/ngboost/). The idea of natural gradient is quite worth exploration. But I haven't got enough time to read through this [paper](https://arxiv.org/pdf/1910.03225.pdf) recently.
10. [CatBoost](https://github.com/catboost). I started looking at [this documentation](https://catboost.ai/en/docs/concepts/algorithm-main-stages_cat-to-numberic) to see how categorical values are transformed into numerical ones in CatBoost. I plan to read the [paper](https://arxiv.org/pdf/1706.09516.pdf) in the near future to further understand the mechanism of this model.
11. Radial Basis Function Network: This topic is waiting to be gone through in the future. Here I only list a few reading sources for self reference later.
  - [Chinese Tutorial](https://zwindr.blogspot.com/2017/08/ml-radial-basis-function-network.html)
12. PyTorch: My major plan in terms of coding this summer would be to systematically learn PyTorch framework (and extend to Tensorflow in the future if time allows).
  - [Official Tutorial](https://pytorch.org/tutorials/)
  - [Chinese Version](https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/)
  - [Github Tutorial I](https://github.com/yunjey/pytorch-tutorial)
  - [Github Tutorial II](https://github.com/MorvanZhou/PyTorch-Tutorial)
13. Neural Networks: I came into this [video series](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) by 3b1b on Youtube. Chapter 4 provides a good explanation of back propagation.
